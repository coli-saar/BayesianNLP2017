\documentclass[11pt]{beamer}
\usetheme{Berkeley}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{forest}

\author{Christoph Teichmann}
\title{The Dirichlet Categorial Model}

\subtitle{}

\logo{}

\institute{}

\date{}

\subject{}

\setbeamercovered{transparent}

\setbeamertemplate{navigation symbols}{}

\begin{document}
	
	\begin{frame}
		\maketitle
	\end{frame}
	
	\section{Goals}
	
	\begin{frame}{Teaching Goals}
		\begin{itemize}
			\item Basic properties of Dirichlet distribution
			\item Dirichlet Categorial model
			\item Chinese Restaurant Process
			\item How to build a simple language model from that
		\end{itemize}
	\end{frame}
	
	\section{Motivation}
	
	\begin{frame}{Categorial Variables in Computationl Linguistics}
		\centering
		We want probabilites for:
				
		\begin{itemize}
			\item Next word given previous ones
			\item Children given parent in constituent tree
			\item Next Part-Of-Speech tag given previous ones
		\end{itemize}
		
		\vspace{20pt} Mary sees $\lbrace$ the, something, John, .\ $\rbrace$ \dots
	\end{frame}
	
	\begin{frame}{Categorial Variables in Computationl Linguistics}
		\centering
		We want probabilites for:
		
		\begin{itemize}
			\item Next word given previous ones
			\item Children given parent in constituent tree
			\item Next Part-Of-Speech tag given previous ones
		\end{itemize}
		
		\begin{tabular}{lr}
			\begin{forest}
				[S [ NP [Mary] ] [VP [ sees ] [NP [\dots] ] ] ]
			\end{forest} &
			\begin{tabular}{c}
				$NP \rightarrow something$ \\
				$NP \rightarrow John$
			\end{tabular}
		\end{tabular}
	\end{frame}
	
	\section{A Simple Model}
	
	\begin{frame}{The Next Word -- The Bayesian Approach}
		\centering
		\begin{itemize}
			\item Turn what you want into a random quantity

			-- probabilities \checkmark
			\item Find data that is connected to what you care about
			
			-- sentences from websites (tokenized) \checkmark
			\item full model that connect random quantity of interest and data
			
			-- uh oh
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Simple Model P(text|Probabilities)}
		\begin{itemize}
			\item N-Gram model
			\item $P(\text{w}_i|\text{w}_{i-1},\dots,\text{w}_{0}) = P(\text{w}_i|\text{w}_{i-1},\dots,\text{w}_{i-(n-1)})$
			\item let us start with $n=1$ / Unigram: $P(\text{w}_i|\text{w}_{i-1},\dots,\text{w}_{0}) = P(\text{w}_i)$ 
		\end{itemize}
		
		This gives model for data given word probabilities -- where do word probabilities come from?
	\end{frame}
	
	\begin{frame}{Simplest Choice -- How to Write Word Probabilities}
		\begin{itemize}
			\item Assume finite lexicon $L$
			\item We will write the word probabilities as $P_w(x)$ for every $w \in L$
			\item $\left( \sum_{x \in L} P_w(x) \right) = 1$
			\item For all $x$ in $L$ $P_w(x) \geq 0$
			\item Write different distributions as $P_{w}^{x}$, $P_{w}^{y}$ \dots
			\item Write prior, i.e.\ distribution over $P_{w}$s as $P_{\text{prior}}$
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Simplest Choice -- Everything Equally Likely}
		\begin{itemize}
			\item Let us start with an uniform model, i.e.\ every $P^{x}_{w}$ as likely as every other one
			\item For every $P^{x}_{w}$ we have $P_{\text{prior}}(P^{x}_{w}) = \frac{1}{Z}$
			\item $Z$ is constant to make sure that all integrates to $1$
			\item Means we think $P^{x}_{w}(the) = 1$ is as likely as a distribution which assumes all words are at least possible
		\end{itemize}
	\end{frame}
	
	\begin{frame}{How Does This Work}
		\begin{itemize}
			\item Lexicon: $L = \lbrace \text{Mary}, \text{sees}, \text{.}, \text{something}, \text{John}  \rbrace$
			\item Dataset: ``Mary sees something ?''
			\item What is the next word? (hint: ``.'')
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Quick Diversion}
		\begin{itemize}
			\item Lexicon: $L = \lbrace \text{Mary}, \text{sees}, \text{.}, \text{something}, \text{John}  \rbrace$
			\item Dataset: ``Mary sees something ?''
			\item What is the next word? (hint: ``.'')
		\end{itemize}
		
		WWMLD -- what would maximum likelihood do? - pick the model that makes the data most likely!
	\end{frame}
	
	\begin{frame}{How Does This Work}
		WWMLD --
		
		\begin{align*}
			P^{ml}_{w}(Mary) & = & \frac{1}{4}\\
			P^{ml}_{w}(sees) & = & \frac{1}{4}\\
			P^{ml}_{w}(something) & = & \frac{1}{4}\\
			P^{ml}_{w}(.) & = & 0 \\
			P^{ml}_{w}(John) & = & 0\\
		\end{align*}		
	\end{frame}
	
	\section{The Dirichlet Distribution}
	
\end{document}